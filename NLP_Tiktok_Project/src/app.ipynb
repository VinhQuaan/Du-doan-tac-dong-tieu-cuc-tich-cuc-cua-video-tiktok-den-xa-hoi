{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl data comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jmespath\n",
    "import asyncio\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "from typing import List, Dict\n",
    "from httpx import AsyncClient, Response\n",
    "from loguru import logger as log\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncClient(\n",
    "    # enable http2\n",
    "    http2=True,\n",
    "    headers={\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"content-type\": \"application/json\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_comments(response: Response) -> Dict:\n",
    "    try:\n",
    "        data = json.loads(response.text)\n",
    "    except json.JSONDecodeError:\n",
    "        log.error(f\"Failed to parse JSON response: {response.text}\")\n",
    "        return {\"comments\": [], \"total_comments\": 0}\n",
    "\n",
    "    comments_data = data.get(\"comments\", [])\n",
    "    total_comments = data.get(\"total\", 0)\n",
    "\n",
    "    if not comments_data:\n",
    "        log.warning(f\"No comments found in response: {response.text}\")\n",
    "        return {\"comments\": [], \"total_comments\": total_comments}\n",
    "\n",
    "    parsed_comments = []\n",
    "    for comment in comments_data:\n",
    "        result = jmespath.search(\n",
    "            \"\"\"{\n",
    "            text: text\n",
    "            }\"\"\",\n",
    "            comment\n",
    "        )\n",
    "        parsed_comments.append(result)\n",
    "    return {\"comments\": parsed_comments, \"total_comments\": total_comments}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_comments(post_id: int, comments_count: int = 20, max_comments: int = None) -> List[Dict]:\n",
    "    \n",
    "    def form_api_url(cursor: int):\n",
    "        base_url = \"https://www.tiktok.com/api/comment/list/?\"\n",
    "        params = {\n",
    "            \"aweme_id\": post_id,\n",
    "            'count': comments_count,\n",
    "            'cursor': cursor # the index to start from      \n",
    "        }\n",
    "        return base_url + urlencode(params)\n",
    "    \n",
    "    log.info(f\"Scraping comments from post ID: {post_id}\")\n",
    "    first_page = await client.get(form_api_url(0))\n",
    "    data = parse_comments(first_page)\n",
    "    comments_data = data[\"comments\"]\n",
    "    total_comments = data[\"total_comments\"]\n",
    "\n",
    "    if not comments_data:\n",
    "        log.warning(f\"No comments found for post ID {post_id}\")\n",
    "        return []\n",
    "    if max_comments and max_comments < total_comments:\n",
    "        total_comments = max_comments\n",
    "\n",
    "    log.info(f\"Scraping comments pagination, remaining {total_comments // comments_count - 1} more pages\")\n",
    "    _other_pages = [\n",
    "        client.get(form_api_url(cursor=cursor))\n",
    "        for cursor in range(comments_count, total_comments + comments_count, comments_count)\n",
    "    ]\n",
    "\n",
    "    for response in asyncio.as_completed(_other_pages):\n",
    "        response = await response\n",
    "        new_comments = parse_comments(response)[\"comments\"]\n",
    "        comments_data.extend(new_comments)\n",
    "        \n",
    "        # If we have reached or exceeded the maximum number of comments to scrape, stop the process\n",
    "        if max_comments and len(comments_data) >= max_comments:\n",
    "            comments_data = comments_data[:max_comments]\n",
    "            break\n",
    "\n",
    "    log.success(f\"Scraped {len(comments_data)} comments from post ID {post_id}\")\n",
    "    return comments_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btrunghieu/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_hidden_state, output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False # Dropout will errors if without this\n",
    "        )\n",
    "\n",
    "        x = self.drop(output)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentimentClassifier(n_classes=3)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('phobert_fold1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btrunghieu/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['CLEAN', 'OFFENSIVE', 'HATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(text, tokenizer, max_len=120):\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, y_pred = torch.max(output, dim=1)\n",
    "\n",
    "    return class_names[y_pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def predict_comments(video_id):\n",
    "    comments = await scrape_comments(\n",
    "        post_id=int(video_id),\n",
    "        max_comments=2000,\n",
    "        comments_count=20\n",
    "    )\n",
    "    predictions = []\n",
    "    for comment in comments:\n",
    "        text = comment['text']\n",
    "        probs = infer(text, tokenizer)\n",
    "        predictions.append({'comment': text, 'predictions': probs})\n",
    "    \n",
    "    # Tính toán tỷ lệ phần trăm của mỗi nhãn\n",
    "    total_comments = len(predictions)\n",
    "    label_counts = [0, 0, 0]  # Assuming there are 3 labels\n",
    "\n",
    "    for prediction in predictions:\n",
    "        probs = prediction['predictions']\n",
    "        if probs == 'CLEAN':\n",
    "            label_counts[0] += 1\n",
    "        elif probs == 'OFFENSIVE':\n",
    "            label_counts[1] += 1\n",
    "        else :\n",
    "            label_counts[2] += 1\n",
    "\n",
    "    label_percentages = [count / total_comments * 100 for count in label_counts]\n",
    "    results = {\n",
    "        'total_comments': total_comments,\n",
    "        'label_percentages': {\n",
    "            'CLEAN': label_percentages[0],\n",
    "            'OFFENSIVE': label_percentages[1],\n",
    "            'HATE': label_percentages[2],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "iface = gr.Interface(\n",
    "    fn=predict_comments,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "Running on public URL: https://3dc7074bce3d53f849.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3dc7074bce3d53f849.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
